{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Linear models",
      "version": "0.3.2",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/adicu/devfest-deeplearning/blob/master/linear_models.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "id": "AT3xf2jN4gKA",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# wip"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "LotyLF2wnkrI",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Linear models"
      ]
    },
    {
      "metadata": {
        "id": "aRoQ_KE5QfOv",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Machine learning is about learning patterns from data. _Deep learning_ is a subset of machine learning, where we'll build neural networks to identify patterns. We'll first need a strong understanding of _linear models_.\n",
        "\n",
        "> ### What's a linear model?\n",
        "> In a _linear model_, each term is either:\n",
        "  - a constant, or\n",
        "  - a product of a parameter and a variable\n",
        "\n",
        "> For example, $ax + b = 0$ and $ax^2 + bx + c = 0$ are both linear models. \n",
        "\n",
        "> Despite the $x^2$ term, $ax^2+bx+c = 0$ is a linear model because the model is linear in the _parameter_ $x^2$: we multiply it by some constant $a$. If this is confusing, imagine setting $x' = x^2$ and substituting it into the model: then our model is $ax' + bx +c = 0$, which is easier to identify as linear.\n",
        "\n",
        "We'll find out soon that the core of all neural networks is a set of nonlinear functions. This nonlinearity makes neural networks - as you might have heard - very powerful. Soon we'll build those nonlinear functions ourselves.\n",
        "\n",
        "First, we need to build a few linear functions. Then we can transform them into a powerful nonlinear form. "
      ]
    },
    {
      "metadata": {
        "id": "YVLvVpmCoio-",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Lines in higher dimensions\n",
        "Most students will be familiar with the equation of a line. For example, $y = -2x$ is an equation for a line. Rewriting this equation lets us generalize the line equation to higher dimensions. First, convert into the normal form, $2x + y = 0$. Then, we can represent the normal form as the dot product $\\big\\langle (2,1), (x,y) \\big\\rangle = 0$.\n",
        "Finally, we can represent all lines passing through any point by adding a _bias_ weight. The _bias_ is a constant; for example, the line equations\n",
        "$$\n",
        "\\big\\langle (2,1), (x,y) \\big\\rangle -1 = 0 \\\\\n",
        "2x + y = 1 \\\\\n",
        "y = -2x + 1\n",
        "$$\n",
        "are equivalent. Finally, we can generalize this linear model to the form most popular in machine learning:\n",
        "$$\n",
        "\\langle w, x \\rangle + b = 0\n",
        "$$\n",
        "which expands to the equation of a _hyperplane_: $w_1x_1 + w_2x_2 + \\dots + w_nx_n + b = 0$. That looks uncoincidentally like our original line equation, but now in $n$ dimensions: in fact, a point is a _hyperplane_ in $1$-dimensional space, a line is a _hyperplane_ in $2$-dimensional space, and a plane is a _hyperplane_ in $3$-dimensional space.\n"
      ]
    },
    {
      "metadata": {
        "id": "FKMWP97KNrqt",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "> ### The hyperplane\n",
        "> _Hyperplane_: A subspace that separates a vector space into two parts. A linear equation for a _hyperplane_ lets us perform classification (for two classes) easily: an input we want to classify as class $0$ or class $1$ is either above or below the _hyperplane_.\n",
        "\n",
        "> Formally, a _hyperplane_ is a subspace of dimension $n-1$ inside an $n$-dimensional space.\n",
        "\n",
        " >> ![](https://i.imgur.com/QCDR8MU.png)\n",
        " >> __Left:__ a line is a hyperplane in $2$-D space. __Right__: a plane is a hyperplane in $3$-D space."
      ]
    },
    {
      "metadata": {
        "id": "06jFswuFtBP0",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Classification using hyperplanes"
      ]
    },
    {
      "metadata": {
        "id": "LCni4yzRt_hH",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "A _decision function_ performs classification: given a point, it classifies that point as belonging to a certain set. \n",
        "\n",
        "Let's define a function $f:\\mathbb{R}^n\\rightarrow \\mathbb{R}$. If you're not familiar with this notation, it just means that $f$ takes an $n$-dimensional input, and outputs a real number. We'll define $f$ using our hyperplane equation.\n",
        "$$\n",
        "f(x) = \\langle w, x \\rangle + b\n",
        "$$\n",
        "Then two points --- let's call them $x_1, x_2$ ---  located on opposite sides of that hyperplane will together satisfy one of the following inequalities:\n",
        "$$\n",
        "f(x_1) < 0 < f(x_2) \\\\\n",
        "f(x_2) < 0 < f(x_1)\n",
        "$$\n",
        "\n",
        "So our _decision function_ could be as concise as $sign\\big(f(x)\\big)$, since that function outputs whether $f(x) > 0$ or $f(x) < 0$."
      ]
    },
    {
      "metadata": {
        "id": "M65fsrr1n95H",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Perceptron"
      ]
    },
    {
      "metadata": {
        "id": "RX1FgjwnoFqb",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "The __perceptron__ is a learning algorithm. It finds a separating _hyperplane_ by minimizing the distance of misclassified points to the _decision boundary_. Here's an example:"
      ]
    },
    {
      "metadata": {
        "id": "8gtfayJdq30i",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Rosenblatt's Perceptron Learning Algorithm\n",
        "```\n",
        "X, y = training_data, training_labels\n",
        "w = [0, 0, ..., 0]\n",
        "b = 0\n",
        "while(any misclassification exists):\n",
        "    for each x in X:\n",
        "    \n",
        "    abuse of notation\n",
        "```"
      ]
    },
    {
      "metadata": {
        "id": "7yyumVrCljwf",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "But linear models have limitations. In the 1980s the perceptron algorithm represented the state-of-the-art in deep learning, but it can't learn the XOR function:\n",
        "\n",
        "\\begin{align*}\n",
        "f\\big([0,1], w\\big) &= 1 \\\\\n",
        "f\\big([1,0], w\\big) &= 1 \\\\\n",
        "f\\big([1,1], w\\big) &= 0 \\\\\n",
        "f\\big([0,0], w\\big) &= 0\n",
        "\\end{align*}\n",
        "\n",
        "Limitations like these resulted in the first AI winter."
      ]
    },
    {
      "metadata": {
        "id": "VEZGvuYUN2fk",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Gradient Descent"
      ]
    }
  ]
}