{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "U1fzcVAP7TYJ"
   },
   "source": [
    "# Backpropagation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "UsVRel2v7VVV"
   },
   "source": [
    "Now that we know what *gradient descent* is, and we have an idea of why we would want the gradient of our *loss function* ($C$) - how does one actually go about computing the gradient of a loss function?\n",
    "\n",
    "The *backprogagation algorithm* is a fast algorithm to compute gradients - without it, none of the neural networks we use today would be able to function efficiently.\n",
    "\n",
    "As touched on earlier, our neural network learns its weights and biases using gradient descent by minimizing loss. The key concept in backpropagation is the partial derivative of the loss function with respect to any weight $w$ or bias $b$ - $\\frac{\\partial C}{\\partial w}$ or $\\frac{\\partial C}{\\partial b}$ - which gives us an idea of how the loss will change when a bias or weight is changed. This is the power of understanding the backpropagation algorithm - it gives us the intuition necessary to manipulate the behaviour of our neural network by changing weights and biases. Understanding backpropagation gives you the key to open up the \"black box\" of a neural network.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Mp3uocit9OqM"
   },
   "source": [
    "## The Motivation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ivSvEzQf9TNN"
   },
   "source": [
    "Before diving into any of the math, lets try and motivate the *need* for backpropagation.\n",
    "\n",
    "Our goal is to see how the loss function changes when we change a given weight. Sounds pretty simple, right? Just change the weight in question,  $w_i$, and calculate the loss $C(w)$. \n",
    "\n",
    "However, what seems like \"just\" changing a single weight quickly snowballs into a series of changes that effectively forces us to recalculate all of the weights in our networks. Let's break that down:\n",
    "\n",
    "1. We change a single weight.\n",
    "![alt text](http://neuralnetworksanddeeplearning.com/images/tikz22.png)\n",
    "\n",
    "2. Th\n",
    "\n",
    "However, changing that single weight changes the output activation of the corresponding neuron, which then causes changes in all of the activations in the next layer, and the next, and so on. So, recalculating this loss function after changing a single weight requires us to pass through the entire neural network again!\n",
    "\n",
    "To make matters worse, consider how many different weights and biases a single neural network can have - millions! All of a sudden, changing each weight/bias individually and recalculating the loss function each time seems a lot more daunting, right? \n",
    "\n",
    "As we'll see, backpropagation allows us to calculate all of the partial derivatives in one pass forward and one pass backward through the neural network. So, instead of calculating the loss function a million times (which requires a million forward passes), we now just need to make a forward and a backward pass. Pretty great!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "gcWXzmiiFbG4"
   },
   "source": [
    "## The Four Fundamental Equations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "BgfdpHT8J208"
   },
   "source": [
    "below, interpret $j$ and $l$ to mean we are working with $j^{th}$ neuron in layer $l$, out of $L$ layers\n",
    "\n",
    "also interpret $s \\odot t$ to be the *elementwise* product of the vectors s and t"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "LyKBw9j1Ffz7"
   },
   "source": [
    "### Equation 1 - error in the output layer\n",
    "\n",
    "$\\delta_j^L=\\frac{\\partial C}{\\partial a_j^L} \\sigma ' (z_j^L)$\n",
    "\n",
    "#### What does this mean?\n",
    "\n",
    "$\\frac{\\partial C}{\\partial a_j^L}$ - how fast does the loss change depending on the $j^{th}$ output activation (the activation of the output in the last layer)\n",
    "\n",
    "$\\sigma'(z_j^L)$- how fast does the activation function $\\sigma$ change at $z_j^L$\n",
    "\n",
    "This is a component-wise equation - we can convert it to the equivalent matrix-based form easily\n",
    "\n",
    "$\\delta^L = \\nabla_a C $ $\\odot$ $\\sigma'(z^L)$\n",
    "\n",
    "but we will use the component-wise equation for convenience.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "s-Fc1F9UK6WV"
   },
   "source": [
    "### Equation 2 - error $\\delta^l$ in terms of the error in the next layer $\\delta^{l+1}$\n",
    "\n",
    "$\\delta^l=((w^{l+1})^T\\delta^{l+1})\\odot \\sigma'(z^l)$\n",
    "\n",
    "#### What does this mean?\n",
    "\n",
    "$(w^{l+1})^T$ - the transpose of the weight matrix $w^{l+1}$ for the $l+1^{th}$ layer\n",
    "\n",
    "We can think of this as moving the error $\\delta^{l+1}$ backward to the output of the $l^{th}$ layer by applying the transpose weight matrix, and then backward through the activation function in layer l (by taking the elementwise product $\\odot \\sigma'(z^l)$) to arrive at $\\delta^l$\n",
    "\n",
    "Thus, we can use Equation 1 to calculate $\\delta^L$, and then use Equation 2 to calculate all the other layers' errors by moving backwards through the network."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "yQay5SEzNyyy"
   },
   "source": [
    "### Equation 3 - rate of change of cost with respect to bias\n",
    "\n",
    "$\\frac{\\partial C}{\\partial b^l_j} = \\delta^l_j$\n",
    "\n",
    "#### What does this mean?\n",
    "\n",
    "The error $\\delta^l_j$ is equal to the rate of change of cost with respect to bias $\\frac{\\partial C}{\\partial b^l_j} $\n",
    "\n",
    "Since given Equation 1 and 2, we can compute any $\\delta^l_j$, we can compute any $\\frac{\\partial C}{\\partial b^l_j}$ as well"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "dU6EcwzHOa8G"
   },
   "source": [
    "### Equation 4 - rate of change of cost with respect to weight\n",
    "\n",
    "$\\frac{\\partial C}{\\partial w^l_{jk}} = a_k^{l-1}\\delta_j^l $\n",
    "\n",
    "or\n",
    "\n",
    "$\\frac{\\partial C}{\\partial w}=a_{\\text{in}}\\delta_{\\text{out}}$\n",
    "\n",
    "#### What does this mean?\n",
    "\n",
    "The rate of change of cost with respect to weight is the product of the activation of the neuron *input* to the weight $w$, and the error of the neuron *output* from the weight $w$.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "pRqZrqDlQfhw"
   },
   "source": [
    "### Some Useful Intuition\n",
    "When the sigmoid function is approximately 0 or 1, it is very flat. In Equation 1, this gives us $\\sigma'(z_j^L)\\approx 0$. Essentially, a weight in the final layer will not change much - will \"learn slowly\" - if the output neuron is either low () or high () activation (in this case, we call the output neuron *saturated*).\n",
    "\n",
    "The above logic also applies for the $\\sigma'(z^l)$ term in Equation 2, so this intuition can be extended to earlier layers.\n",
    "\n",
    "Finally in Equation 4, if the activation of a neuron is small $(\\approx 0)$, then the gradient term will also be small. Thus the weight will not change much during gradient descent - it will \"learn slowly\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "39oyP8CtX--x"
   },
   "source": [
    "## The Backpropagation Algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "HfWoNrTwYGvL"
   },
   "source": [
    "1. Input x: Set the activation $a^1$ according to the input.\n",
    "\n",
    "2. Feedforward: For each layer $l = 2, 3, ..., L$ compute $z^l = w^l a^l-1 + b^l$ and $a^l = \\sigma(z^l)$\n",
    "\n",
    "3. Output error $\\delta^L$: compute $\\delta^L$ (Equation 1)\n",
    "\n",
    "4. Backpropagate the error: compute  $\\delta^l$ for all the earlier layers (Equation 2)\n",
    "\n",
    "5. Output: The gradient of the cost function is given by Equations 3 and 4.\n",
    "\n",
    "Step 4 is why the algorithm is called *back*propagation - we compute the error vectors backward, starting from the final layer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "PjYkJ4t_Zn8E"
   },
   "source": [
    "## Additional Resources"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "cnsKsS8LZqhA"
   },
   "source": [
    "The material above is essentially a condensed version of Chapter 2 of Michael Nielsen's wonderful (and free!) [Neural Networks and Deep Learning textbook](http://neuralnetworksanddeeplearning.com/chap2.html). \n",
    "\n",
    "\n",
    "\n",
    "For some visual intuition of what's going on in backpropagation, check out 3Blue1Brown's [video](https://www.youtube.com/watch?v=Ilg3gGewQ5U). For a little bit more math, he has a [follow up](https://www.youtube.com/watch?v=tIeHLnjs5U8).\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "deep_learning_backprop",
   "provenance": [],
   "toc_visible": true,
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
