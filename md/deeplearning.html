<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html xmlns="http://www.w3.org/1999/xhtml">
<head>
  <meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
  <meta http-equiv="Content-Style-Type" content="text/css" />
  <meta name="generator" content="pandoc" />
  <title></title>
  <style type="text/css">code{white-space: pre;}</style>
  <style type="text/css">
div.sourceCode { overflow-x: auto; }
table.sourceCode, tr.sourceCode, td.lineNumbers, td.sourceCode {
  margin: 0; padding: 0; vertical-align: baseline; border: none; }
table.sourceCode { width: 100%; line-height: 100%; }
td.lineNumbers { text-align: right; padding-right: 4px; padding-left: 4px; color: #aaaaaa; border-right: 1px solid #aaaaaa; }
td.sourceCode { padding-left: 5px; }
code > span.kw { color: #007020; font-weight: bold; } /* Keyword */
code > span.dt { color: #902000; } /* DataType */
code > span.dv { color: #40a070; } /* DecVal */
code > span.bn { color: #40a070; } /* BaseN */
code > span.fl { color: #40a070; } /* Float */
code > span.ch { color: #4070a0; } /* Char */
code > span.st { color: #4070a0; } /* String */
code > span.co { color: #60a0b0; font-style: italic; } /* Comment */
code > span.ot { color: #007020; } /* Other */
code > span.al { color: #ff0000; font-weight: bold; } /* Alert */
code > span.fu { color: #06287e; } /* Function */
code > span.er { color: #ff0000; font-weight: bold; } /* Error */
code > span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
code > span.cn { color: #880000; } /* Constant */
code > span.sc { color: #4070a0; } /* SpecialChar */
code > span.vs { color: #4070a0; } /* VerbatimString */
code > span.ss { color: #bb6688; } /* SpecialString */
code > span.im { } /* Import */
code > span.va { color: #19177c; } /* Variable */
code > span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code > span.op { color: #666666; } /* Operator */
code > span.bu { } /* BuiltIn */
code > span.ex { } /* Extension */
code > span.pp { color: #bc7a00; } /* Preprocessor */
code > span.at { color: #7d9029; } /* Attribute */
code > span.do { color: #ba2121; font-style: italic; } /* Documentation */
code > span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code > span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code > span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
  </style>
  <script src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML" type="text/javascript"></script>
</head>
<body>
<h1 id="linear-models">Linear models</h1>
<p>Machine learning is about learning patterns from data. <em>Deep learning</em> is a subset of machine learning, where we'll build neural networks to identify patterns. We'll first need a strong understanding of <em>linear models</em>.</p>
<blockquote>
<h3 id="whats-a-linear-model">What's a linear model?</h3>
<p>A <em>linear model</em> is simply an equation where each term is either: - a constant, or - a product of a parameter and a variable</p>
</blockquote>
<blockquote>
<p>For example, <span class="math inline">\(ax + b = 0\)</span> and <span class="math inline">\(ax^2 + bx + c = 0\)</span> are both linear models where <span class="math inline">\(x\)</span> is a variable and <span class="math inline">\(a, b, c\)</span> are constant terms.</p>
</blockquote>
<blockquote>
<p>Despite the <span class="math inline">\(x^2\)</span> term, <span class="math inline">\(ax^2+bx+c = 0\)</span> is a linear model because the model is linear in the <em>parameter</em> <span class="math inline">\(x^2\)</span>: we multiply it by some constant <span class="math inline">\(a\)</span>. If this is confusing, imagine setting <span class="math inline">\(x&#39; = x^2\)</span> and substituting it into the model: then our model is <span class="math inline">\(ax&#39; + bx +c = 0\)</span>, which is easier to identify as linear.</p>
</blockquote>
<p>We'll find out soon that the core of all neural networks is a set of nonlinear functions. This nonlinearity makes neural networks - as you might have heard - very powerful. Soon we'll build those nonlinear functions ourselves.</p>
<p>First, we need to build a few linear functions. Then we can transform them into a powerful nonlinear form.</p>
<h2 id="lines-in-higher-dimensions">Lines in higher dimensions</h2>
<p>Most students will be familiar with the equation of a line. For example, <span class="math inline">\(y = -2x\)</span> is an equation for a line. Rewriting this equation lets us generalize the line equation to higher dimensions. First, convert into the normal form, <span class="math inline">\(2x + y = 0\)</span>. Then, we can represent the normal form as the dot product <span class="math inline">\(\big\langle (2,1), (x,y) \big\rangle = 0\)</span>. Finally, we can represent all lines passing through any point by adding a <em>bias</em> weight. The <em>bias</em> is a constant; for example, the line equations <span class="math display">\[
\big\langle (2,1), (x,y) \big\rangle -1 = 0 \\
2x + y = 1 \\
y = -2x + 1
\]</span> are equivalent. Finally, we can generalize this linear model to the form most popular in machine learning: <span class="math display">\[
w^Tx + b = 0
\]</span> which expands to the equation of a <em>hyperplane</em>: <span class="math inline">\(w_1x_1 + w_2x_2 + \dots + w_nx_n + b = 0\)</span>. That looks uncoincidentally like our original line equation, but now in <span class="math inline">\(n\)</span> dimensions: in fact, a point is a <em>hyperplane</em> in <span class="math inline">\(1\)</span>-dimensional space, a line is a <em>hyperplane</em> in <span class="math inline">\(2\)</span>-dimensional space, and a plane is a <em>hyperplane</em> in <span class="math inline">\(3\)</span>-dimensional space.</p>
<blockquote>
<h3 id="the-hyperplane">The hyperplane</h3>
<p><em>Hyperplane</em>: A subspace that separates a vector space into two parts. A linear equation for a <em>hyperplane</em> lets us perform classification (for two classes) easily: an input we want to classify as class <span class="math inline">\(0\)</span> or class <span class="math inline">\(1\)</span> is either above or below the <em>hyperplane</em>.</p>
</blockquote>
<blockquote>
<p>Formally, a <em>hyperplane</em> is a subspace of dimension <span class="math inline">\(n-1\)</span> inside an <span class="math inline">\(n\)</span>-dimensional space.</p>
</blockquote>
<blockquote>
<blockquote>
<p><img src="https://i.imgur.com/QCDR8MU.png" width="1200px" /> <strong>Left:</strong> a line is a hyperplane in <span class="math inline">\(2\)</span>-D space. <strong>Right</strong>: a plane is a hyperplane in <span class="math inline">\(3\)</span>-D space.</p>
</blockquote>
</blockquote>
<h3 id="classification-using-hyperplanes">Classification using hyperplanes</h3>
<p>A <em>decision function</em> performs classification: given a point, it classifies that point as belonging to a certain set.</p>
<p>Let's define a function <span class="math inline">\(f:\mathbb{R}^n\rightarrow \mathbb{R}\)</span>. If you're not familiar with this notation, it just means that <span class="math inline">\(f\)</span> takes an <span class="math inline">\(n\)</span>-dimensional input, and outputs a real number. We'll define <span class="math inline">\(f\)</span> using our hyperplane equation. <span class="math display">\[
f(x) = w^T x + b
\]</span> Then two points --- let's call them <span class="math inline">\(x_1, x_2\)</span> --- located on opposite sides of that hyperplane will together satisfy one of the following inequalities: <span class="math display">\[
f(x_1) &lt; 0 &lt; f(x_2) \\
f(x_2) &lt; 0 &lt; f(x_1)
\]</span></p>
<p>So our <em>decision function</em> could be as concise as <span class="math inline">\(sign\big(f(x)\big)\)</span>, since that function outputs whether <span class="math inline">\(f(x) &gt; 0\)</span> or <span class="math inline">\(f(x) &lt; 0\)</span>.</p>
<h2 id="perceptron-algorithm">Perceptron algorithm</h2>
<p>The <strong>perceptron</strong> is a learning algorithm. It finds a separating <em>hyperplane</em> by minimizing the distance of misclassified points to the <em>decision boundary</em>.</p>
<h3 id="training">Training</h3>
<p>All machine learning models need to be trained (fitted) to adjust their parameters and learn from a dataset. The perceptron is no exception. A perceptron finds its separating hyperplane <span class="math inline">\(f(x) = w^T x + b\)</span> by repeating two steps: 1. Check if the separating hyperplane misclassified (made a mistake on) any point. 2. If so, <em>update</em> the coefficient vector (<span class="math inline">\(w\)</span>) and bias (<span class="math inline">\(b\)</span>) to remove that mistake.</p>
<p>The algorithm stops once all points are correctly classified. We can <em>update</em> <span class="math inline">\(w\)</span> and <span class="math inline">\(b\)</span> to fix a mistaken classification using the following rule:</p>
<h4 id="perceptron-update-rule">Perceptron update rule</h4>
<p>Let $x= $ an input point (e.g. a column vector), and $y {0, 1}= $ the ground truth label for that point. <span class="math inline">\(f(x)\)</span> satisfies one of three conditions: 1. If <span class="math inline">\(f(x) = y\)</span>, then <span class="math inline">\(f(x)\)</span> made no mistake; no change. 2. If <span class="math inline">\(f(x) = 1\)</span> but <span class="math inline">\(y=0\)</span>, then <span class="math inline">\(w \leftarrow w-x\)</span>. 3. If <span class="math inline">\(f(x) = 0\)</span> but <span class="math inline">\(y=1\)</span>, then <span class="math inline">\(w \leftarrow w+x\)</span>.</p>
<p>We can condense this update rule further by observing that the expression <span class="math inline">\(\big(y-f(x)\big)\in \{ -1, 1\}\)</span> whenever <span class="math inline">\(f(x)\)</span> has made a mistake. Condensing (2.) and (3.):</p>
<ol style="list-style-type: decimal">
<li>If <span class="math inline">\(f(x) = y\)</span>, no change</li>
<li>Else <span class="math inline">\(w \leftarrow w + \big(y-f(x)\big)\cdot x\)</span></li>
</ol>
<p>This update rule pushes the hyperplane closer to any point misclassified by the perceptron. Perceptron convergence is guaranteed for any <em>linearly separable</em> dataset.</p>
<h4 id="simplifying-out-b">Simplifying out <span class="math inline">\(b\)</span></h4>
<p>You'll notice we omitted any mention of the bias term <span class="math inline">\(b\)</span> in the update rule. For simplicity, linear models often append the bias term to the weight vector <span class="math inline">\(w\)</span>. Appending <span class="math inline">\(1\)</span> to an input point <span class="math inline">\(x\)</span> lets us effectively compute the bias term using our dot product:</p>
\begin{align*}
w := \begin{bmatrix}w_0\\w_1\\ \vdots \\ w_n \end{bmatrix} \quad x := \begin{bmatrix} x_0\\x_1\\ \vdots \\ x_n\end{bmatrix} &amp; \quad \quad \quad
w&#39; := \begin{bmatrix}w_0\\w_1\\ \vdots \\ w_n \\ b \end{bmatrix} \quad x&#39; := \begin{bmatrix} x_0\\x_1\\ \vdots \\ x_n \\ 1\end{bmatrix} \\ \\
\langle w&#39;, x&#39; \rangle &amp;= w_0 x_0 + w_1 x_1 + \dots + w_n x_n + b(1) \\
&amp;= \langle w, x \rangle + b
\end{align*}
<h3 id="psuedocode">Psuedocode</h3>
<pre><code>X, y = training_data, training_labels
w = [0, 0, ..., 0]
b = 0
while(any points misclassified):
    for each (x, y) in (X, Y):
         f = w^T * x
         w = w + (y-f)*x</code></pre>
<p>This version of the perceptron algorithm is considered the <em>Rosenblatt perceptron</em>. Other variations of the perceptron allow you to set a <em>hyperparameter</em> <span class="math inline">\(\eta\)</span> (<em>eta</em>) which controls the rate of convergence of the separating hyperplane. Deep learning texts often refer to <span class="math inline">\(\eta\)</span> as the <em>learning rate</em>, since the <span class="math inline">\(\eta\)</span> term directly affects the rate at which an algorithm updates.</p>
<p>The learning rate <span class="math inline">\(\eta\)</span> does this by scaling the update rule: instead of setting <span class="math inline">\(w \leftarrow w+\big(y - f(x)\big)\cdot x\)</span>, we set <span class="math inline">\(w \leftarrow w + \eta\big(y-f(x)\big)\cdot x\)</span>. A larger <span class="math inline">\(\eta\)</span> (e.g. $&gt; 1 $) will tend to converge to a separating hyperplane faster, but also risks converging more slowly due to overshooting the decision boundary. A smaller <span class="math inline">\(\eta\)</span> (e.g. <span class="math inline">\(\eta &lt; 1\)</span>) will potentially find a more optimal separating hyperplane (i.e. one that maximizes distance to all points), but will also take longer to converge as we need to perform more updates.</p>
<p>We'll encounter this tradeoff between speed and cost again very soon, especially while learning about <em>gradient descent</em>.</p>
<blockquote>
<h3 id="hyperparameters">Hyperparameters</h3>
<p>Neural networks by definition 'learn' parameters (weights) from training data. We can configure how a neural network learns by setting <em>hyperparameters</em>, which are continuous/integer-valued values manually set prior to training a model. The perceptron learning rate term <span class="math inline">\(\eta\)</span> is a hyperparameter set by you prior to the learning process.</p>
</blockquote>
<h3 id="implementation">Implementation</h3>
<p>Here's a quick <code>numpy</code> implementation.</p>
<div class="sourceCode"><pre class="sourceCode python"><code class="sourceCode python"><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt
<span class="im">import</span> numpy <span class="im">as</span> np</code></pre></div>
<div class="sourceCode"><pre class="sourceCode python"><code class="sourceCode python">X <span class="op">=</span> np.array([
    [<span class="fl">0.8</span>, <span class="fl">0.4</span>],
    [<span class="fl">0.3</span>, <span class="fl">0.1</span>],
    [<span class="fl">0.8</span>, <span class="fl">0.8</span>],
    [<span class="fl">0.4</span>, <span class="fl">0.6</span>],
    [<span class="fl">0.6</span>, <span class="fl">0.8</span>],
    [<span class="fl">0.4</span>, <span class="fl">0.2</span>],
    [<span class="fl">0.4</span>, <span class="fl">0.5</span>],
])
Y <span class="op">=</span> np.array([<span class="dv">0</span>, <span class="dv">0</span>, <span class="dv">1</span>, <span class="dv">1</span>, <span class="dv">1</span>, <span class="dv">0</span>, <span class="dv">1</span>])</code></pre></div>
<div class="sourceCode"><pre class="sourceCode python"><code class="sourceCode python"><span class="kw">def</span> plot_points(X, Y, ax, c_pos<span class="op">=</span><span class="st">&#39;b&#39;</span>, c_neg<span class="op">=</span><span class="st">&#39;r&#39;</span>):
    <span class="cf">for</span> i, x <span class="op">in</span> <span class="bu">enumerate</span>(X):
        ax.scatter(x[<span class="dv">0</span>], x[<span class="dv">1</span>], s<span class="op">=</span><span class="dv">120</span>, 
                   marker<span class="op">=</span>(<span class="st">&#39;_&#39;</span> <span class="cf">if</span> Y[i] <span class="op">&lt;=</span> <span class="dv">0</span> <span class="cf">else</span> <span class="st">&#39;+&#39;</span>), linewidths<span class="op">=</span><span class="dv">2</span>, 
                   c<span class="op">=</span>(c_neg <span class="cf">if</span> Y[i] <span class="op">&lt;=</span> <span class="dv">0</span> <span class="cf">else</span> c_pos))</code></pre></div>
<div class="sourceCode"><pre class="sourceCode python"><code class="sourceCode python">fig, ax <span class="op">=</span> plt.subplots(figsize<span class="op">=</span>(<span class="dv">6</span>, <span class="dv">6</span>))
plot_points(X, Y, ax)</code></pre></div>
<div class="figure">
<img src="output_14_0.png" alt="png" />
</div>
<div class="sourceCode"><pre class="sourceCode python"><code class="sourceCode python"><span class="kw">def</span> train(X, Y):
    w <span class="op">=</span> np.zeros(X.shape[<span class="dv">1</span>]<span class="op">+</span><span class="dv">1</span>)
    epochs <span class="op">=</span> <span class="dv">100</span>
    <span class="cf">for</span> e <span class="op">in</span> <span class="bu">range</span>(epochs):  
        <span class="cf">for</span> x, y <span class="op">in</span> <span class="bu">zip</span>(X, Y):
            pred <span class="op">=</span> np.where((np.dot(w[:<span class="dv">2</span>], x)<span class="op">+</span>w[<span class="dv">2</span>]) <span class="op">&gt;=</span> <span class="fl">0.0</span>, <span class="dv">1</span>, <span class="dv">0</span>)
            w[:<span class="dv">2</span>] <span class="op">+=</span> (y<span class="op">-</span>pred) <span class="op">*</span> x
            w[<span class="dv">2</span>] <span class="op">+=</span> (y<span class="op">-</span>pred)
    <span class="cf">return</span> w

<span class="kw">def</span> predict(w, x):
    <span class="cf">return</span> np.where((np.dot(w[:<span class="dv">2</span>], x)<span class="op">+</span>w[<span class="dv">2</span>]) <span class="op">&gt;</span> <span class="fl">0.0</span>, <span class="dv">1</span>, <span class="dv">0</span>)</code></pre></div>
<div class="sourceCode"><pre class="sourceCode python"><code class="sourceCode python">w <span class="op">=</span> train(X, Y)</code></pre></div>
<div class="sourceCode"><pre class="sourceCode python"><code class="sourceCode python">fig, ax <span class="op">=</span> plt.subplots(figsize<span class="op">=</span>(<span class="dv">6</span>, <span class="dv">6</span>))
<span class="cf">for</span> a <span class="op">in</span> <span class="bu">range</span>(<span class="dv">0</span>, <span class="dv">50</span>):
    <span class="cf">for</span> b <span class="op">in</span> <span class="bu">range</span>(<span class="dv">0</span>, <span class="dv">50</span>):
        i, j <span class="op">=</span> a<span class="op">/</span><span class="dv">50</span>, b<span class="op">/</span><span class="dv">50</span>
        p <span class="op">=</span> predict(w, [i, j])
        ax.scatter(i, j, s<span class="op">=</span><span class="dv">120</span>, marker<span class="op">=</span>(<span class="st">&#39;_&#39;</span> <span class="cf">if</span> p <span class="op">&lt;=</span> <span class="dv">0</span> <span class="cf">else</span> <span class="st">&#39;+&#39;</span>), linewidths<span class="op">=</span><span class="dv">2</span>,
                   c<span class="op">=</span>(<span class="st">&#39;r&#39;</span> <span class="cf">if</span> p <span class="op">&lt;=</span> <span class="dv">0</span> <span class="cf">else</span> <span class="st">&#39;b&#39;</span>), alpha<span class="op">=</span><span class="fl">0.5</span>
                   )
plot_points(X, Y, ax, c_pos<span class="op">=</span><span class="st">&#39;y&#39;</span>, c_neg<span class="op">=</span><span class="st">&#39;y&#39;</span>)</code></pre></div>
<div class="figure">
<img src="output_17_0.png" alt="png" />
</div>
<p>But linear models have limitations. In the 1980s the perceptron algorithm represented the state-of-the-art in deep learning, but it can't learn the XOR function:</p>
\begin{align*}
f\big([0,1], w\big) &amp;= 1 \\
f\big([1,0], w\big) &amp;= 1 \\
f\big([1,1], w\big) &amp;= 0 \\
f\big([0,0], w\big) &amp;= 0
\end{align*}
<p>Limitations like these resulted in the first AI winter.</p>
<h1 id="activation-functions">Activation functions</h1>
<h2 id="motivation">Motivation</h2>
<p>Deep learning is fundamentally a method of scalably building and training nonlinear models on large datasets. We've already achieved robust linear model performance for linearly separable datasets, using the Perceptron algorithm. To make these models useful in more complex datasets, we need to build in nonlinearity.</p>
<p>An <em>activation function</em> (represented commonly and here as <span class="math inline">\(\phi\)</span>) takes the output value of a linear function such as <span class="math inline">\(w^T x + b\)</span> and returns a nonlinear transformation of that value. This behavior has biological origins:</p>
<h3 id="biological-neurons">Biological neurons</h3>
<p>Biological neurons receive signals via <em>dendrites</em> and output signals via an <em>axon</em>, which in turn passes signals across synapses to the <em>dendrites</em> of other neurons. Neurons <em>fire</em> by sending a voltage spike along the axon. The frequency of voltage spikes along the axon is a neuron's <em>firing rate</em>. The deep learning analogue of a neuron's <em>firing rate</em> is an <em>activation function</em>:</p>
<table>
<thead>
<tr class="header">
<th align="left">Biological</th>
<th align="left">Artificial</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="left">Neuron firing rate</td>
<td align="left">Activation function <span class="math inline">\(\phi\)</span></td>
</tr>
<tr class="even">
<td align="left">Voltage spike</td>
<td align="left"><span class="math inline">\(\phi(x)\)</span></td>
</tr>
<tr class="odd">
<td align="left">Axon signal</td>
<td align="left"><span class="math inline">\(x\)</span></td>
</tr>
<tr class="even">
<td align="left">Synaptic strength</td>
<td align="left"><span class="math inline">\(w^T\)</span></td>
</tr>
</tbody>
</table>
<h2 id="fundamental-equations">Fundamental equations</h2>
<p>In deep learning, an activation function $:   $ is usually set to one of the following.</p>
<h3 id="logistic-sigmoid">Logistic sigmoid</h3>
<p><span class="math display">\[
\sigma(x) = \frac{1}{1+e^{-x}}
\]</span> The <em>sigmoid</em> non-linearity takes an input <span class="math inline">\(x \in \mathbb{R}\)</span> and squashes it into an output $ 0 (x) 1$. The sigmoid <em>saturates</em> with a very positive or very negative <span class="math inline">\(x\)</span>; at extreme values of <span class="math inline">\(x\)</span>, it becomes flat and varies only very slightly in response to small changes of <span class="math inline">\(x\)</span>.</p>
<div class="sourceCode"><pre class="sourceCode python"><code class="sourceCode python"><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt
<span class="im">import</span> numpy <span class="im">as</span> np

X <span class="op">=</span> np.arange(<span class="op">-</span><span class="dv">10</span>., <span class="dv">10</span>., <span class="fl">0.1</span>)
sigmoid <span class="op">=</span> <span class="kw">lambda</span> x: <span class="dv">1</span><span class="op">/</span>(<span class="dv">1</span><span class="op">+</span>np.exp(<span class="op">-</span>x))
plt.plot(X, [sigmoid(x) <span class="cf">for</span> x <span class="op">in</span> X], label<span class="op">=</span><span class="st">&#39;$\sigma(x)$&#39;</span>)
plt.legend()<span class="op">;</span></code></pre></div>
<div class="figure">
<img src="output_3_0.png" alt="png" />
</div>
<h3 id="hyperbolic-tangent">Hyperbolic tangent</h3>
\begin{align*}
tanh(x) &amp;= \frac{e^x - e^{-x}}{e^x + e^{-x}}
\end{align*}
The <em>tanh</em> non-linearity also takes an input <span class="math inline">\(x \in \mathbb{R}\)</span> and squashes it into an output $ -1 (x) 1$. Like the logistic sigmoid, <em>tanh</em> saturates at extreme values of <span class="math inline">\(x\)</span>. Unlike the logistic sigmoid, the <em>tanh</em> function is centered at <span class="math inline">\(0\)</span>. This makes sense because the <em>tanh</em> function is actually just a scaled sigmoid:
\begin{align*}
tanh(x) &amp;= \frac{e^x - e^{-x}}{e^x + e^{-x}} \\
&amp;= \frac{1-e^{-2x}}{1+e^{-2x}} \\
&amp;= \frac{2}{1+e^{-2x}}-1 \\
&amp;= 2\sigma(2x)-1
\end{align*}
<div class="sourceCode"><pre class="sourceCode python"><code class="sourceCode python"><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt
<span class="im">import</span> numpy <span class="im">as</span> np

X <span class="op">=</span> np.arange(<span class="op">-</span><span class="dv">10</span>., <span class="dv">10</span>., <span class="fl">0.1</span>)
<span class="kw">def</span> tanh(x):
  u <span class="op">=</span> np.exp(x)
  v <span class="op">=</span> <span class="dv">1</span><span class="op">/</span>u
  <span class="cf">return</span> (u<span class="op">-</span>v)<span class="op">/</span>(u<span class="op">+</span>v)
  
plt.plot(X, [sigmoid(x) <span class="cf">for</span> x <span class="op">in</span> X], label<span class="op">=</span><span class="st">&#39;$\sigma(x)$&#39;</span>)
plt.plot(X, [tanh(x) <span class="cf">for</span> x <span class="op">in</span> X], label<span class="op">=</span><span class="st">&#39;$tanh(x)$&#39;</span>)
plt.legend()<span class="op">;</span></code></pre></div>
<div class="figure">
<img src="output_5_0.png" alt="png" />
</div>
<h3 id="rectified-linear-unit">Rectified linear unit</h3>
<p><span class="math display">\[
g(x) = max(0, x)
\]</span></p>
<p>The rectified linear unit (<em>ReLU</em>) thresholds the activation at <span class="math inline">\(0\)</span>. The <em>ReLU</em> function, consisting of no exponential operations, is easier and faster to compute than the logistic sigmoid or hyperbolic tangent activation functions. The <em>ReLU</em> also does not saturate at extreme values, instead simply behaving linearly.</p>
<div class="sourceCode"><pre class="sourceCode python"><code class="sourceCode python"><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt
<span class="im">import</span> numpy <span class="im">as</span> np

X <span class="op">=</span> np.arange(<span class="op">-</span><span class="dv">5</span>., <span class="dv">5</span>., <span class="fl">0.1</span>)
relu <span class="op">=</span> <span class="kw">lambda</span> x: <span class="bu">max</span>(<span class="dv">0</span>, x)
  
plt.plot(X, [sigmoid(x) <span class="cf">for</span> x <span class="op">in</span> X], label<span class="op">=</span><span class="st">&#39;$\sigma(x)$&#39;</span>)
plt.plot(X, [tanh(x) <span class="cf">for</span> x <span class="op">in</span> X], label<span class="op">=</span><span class="st">&#39;$tanh(x)$&#39;</span>)
plt.plot(X, [relu(x) <span class="cf">for</span> x <span class="op">in</span> X], label<span class="op">=</span><span class="st">&#39;$ReLU(x)$&#39;</span>)
plt.legend()<span class="op">;</span></code></pre></div>
<div class="figure">
<img src="output_7_0.png" alt="png" />
</div>
<h3 id="usage">Usage</h3>
<p>In <a href="TODO">Linear models</a> we discussed the behavior of a a linear model defined by a weight vector <span class="math inline">\(w\)</span>, a bias term <span class="math inline">\(b\)</span>, and an input vector <span class="math inline">\(x\)</span>: <span class="math display">\[
w^T x + b
\]</span> We can pass the output of this linear model to any one of our activation functions above: <span class="math display">\[
h = \phi(w^T x + b)
\]</span> and now have achieved nonlinear behavior.</p>
<h1 id="feedforward-networks">Feedforward networks</h1>
<p>Now that we have covered perceptrons and activation functions, we can put them together to create <em>feedforward neural networks</em>.</p>
<div class="figure">
<img src="https://cs.stanford.edu/people/eroberts/courses/soco/projects/neural-networks/Architecture/images/feedforward.jpg" />

</div>
<p>These networks are important in deep learning models to approximate functions. They're called &quot;feedforward&quot; because information is passed forward through the network and is not passed backwards. (When information is also passed backwards, the network is called a recurrent neural network).</p>
<h3 id="structure">Structure</h3>
<p>Feedforward nets (short for networks) consist of <em>layers</em> of perceptrons. Perceptrons are commonly called nodes in the context of neural networks.The simplest feedforward net consists of a single layer of nodes.</p>
<p>All nodes in a <em>layer</em> are connected to every other node in the previous layer, though different connections may have different weights. Information (data) is passed through each layer of nodes until it gets output through the output layer. <em>Hidden layers</em> are all layers before the <em>output layer</em>.</p>
<div class="figure">
<img src="http://www.fon.hum.uva.nl/praat/manual/Feedforward_neural_networks_1__What_is_a_feedforward_ne_1.png" />

</div>
<p>In the image above, there are three inputs (shown as hollow circles), which are passed into a hidden layer with 4 nodes, and the outputs of that layer are passed as inputs into the final output layer, with 5 nodes. Each connection, additionally, has its own weight. We would say this network has two layers.</p>
<h4 id="image-source">Image source</h4>
<p>http://www.fon.hum.uva.nl/praat/manual/Feedforward_neural_networks_1__What_is_a_feedforward_ne.html</p>
<h2 id="loss-functions"># Loss Functions</h2>
<h2 id="motivation-1">Motivation</h2>
<p>At this point, you should understand how perceptrons can pass information through a neural network. If a network of perceptrons is the performer on stage delivering the flashy results of deep learning, loss functions are the critics in the audience who can evaluate the result with a standard. As we will see in the next section, networks take the feedback produced by loss functions in order to update their weights (<em>Note</em>: This implies that we have talked about weights in the feed forward section) and produce incrementally better output.</p>
<h2 id="purpose-of-a-loss-function">Purpose of a Loss Function</h2>
<p>A loss function provides a measure of the accuracy of the network. We usually have some function <span class="math inline">\(g(a)\)</span> that takes as input some neural network <span class="math inline">\(a\)</span> and compares the output of <span class="math inline">\(a\)</span> over all input values <span class="math inline">\(x\)</span> to the true label of <span class="math inline">\(x\)</span>. When <span class="math inline">\(g \approx 0\)</span> then our network gives very accurate estimates. In this section, we provide two examples of common loss functions and the associated intuition. In the next section on gradient descent, we will see our loss functions allow networks to learn.</p>
<h2 id="mean-squared-error-mse-l_2-loss">Mean Squared Error (MSE, <span class="math inline">\(L_2\)</span> loss)</h2>
<p><span class="math display">\[g(a) = \frac{1}{2n} \sum_{x} \|y(x) - a(x)\|^2\]</span> where <span class="math inline">\(y(x)\)</span> is the true label of input value <span class="math inline">\(x\)</span> and <span class="math inline">\(n\)</span> is the number of samples. The MSE simple finds the normed difference between the true value and predicted value of an input value as represented by the summand <span class="math inline">\(\|y(x) - a(x)\|^2\)</span>. Remember that the output of the neural network and the true labels are <strong>vectors</strong> that correspond to a probability distribution over each possible label. Thus, if our network performs well, we expect that the normed distance between <span class="math inline">\(a(x)\)</span> and <span class="math inline">\(y(x)\)</span> will be close to 0. Taking the summation over all possible <span class="math inline">\(x\)</span> and normalizing over <span class="math inline">\(2n\)</span> gives us the final error value.</p>
<h2 id="cross-entropy">Cross Entropy</h2>
<p><span class="math display">\[g(a) = -\frac{1}{n} \sum_x [y(x) \ln a(x)  + (1 - y(x))\ln ( 1- a(x))]\]</span> Cross Entropy loss is one of the most popular loss functions used in modern Deep Learning architectures. At first glance cross entropy loss makes a lot less intuitive sense than MSE – it isn't even clear that this is a proper loss function. We shall see in the next section, however, that the first derivative of Cross Entropy has some nice properties that give it a &quot;good&quot; learning rate. <strong>NOTE:</strong> Cross Entropy, from a high level point of view, computes the difference in information needed to express the true distribution of labels from the predicted distribution of labels. Further we see that 1. Cross Entropy is always greater than 0. 2. When <span class="math inline">\(a(x)\)</span> approaches <span class="math inline">\(y(x)\)</span> Cross Entropy tends to zero.</p>
<p>Note that these two properties are also characteristic of the MSE.</p>
<h2 id="conclusion">Conclusion</h2>
<p>The important takeaway from this section is to understand the <a href="#purpose-of-a-loss-function">purpose</a> of the loss function. In the next section we will see how we can find the gradient of a loss function in order to &quot;teach&quot; our neural network. We will then introduce backpropagation, the key idea that enables learning to efficiently and powerfully propagate thorughout all layers of our network!</p>
<hr />
<p><em>Notes</em>: LF really need to be understood in the context of gradient descent so not sure if we should introduce them <strong>before</strong> or <strong>after</strong> <span class="citation">@Jessie</span>'s section. Conceptually, loss functions as an isolated concept is pretty simple – they are a set of (preferably smooth) functions that evaluate the effectiveness of an algorithms performance on a specific problem. Seems like it'll be a relatively small section</p>
<h1 id="gradient-descent">Gradient Descent</h1>
<p>Most deep learning algorithms involve some sort of optimization. We'd like to find the value of <span class="math inline">\(x\)</span> where the <em>loss function</em> <span class="math inline">\(f(x)\)</span> is minimized, so the output of our network is approximately the output of the function we are representing.</p>
<p>Gradient descent is the most popular method for optimization, because it works with most functions and is relatively easy to implement. The name gradient descent gives away what the method is - &quot;descending&quot; by moving in the direction of most negative slope. The gradient can be thought of as the derivative of a function (but it is a vector field).</p>
<p>Gradient descent is a way to find a minimum point of a function by starting at a point on the function and making many small moves towards a better (smaller) point. Each of these moves is along the direction of most negative gradient, which gets us to the smallest point possible from our starting point. However, we aren't looking for what the minimum is, but what point the minimum occurs at.</p>
<p>Conceptually, one way to think about gradient descent is to think about walking downhill. In this case, you can only take steps of a fixed size. Additionally, every step you take must be in the direction where the slope is steepest.</p>
<div class="figure">
<img src="http://www.deepideas.net/wp-content/uploads/2017/08/gradient_descent_2.png" />

</div>
<p>Mathematically, gradient descent looks like this: <span class="math inline">\(x&#39; = x - \epsilon \nabla_x f(x)\)</span>. <span class="math inline">\(x&#39;\)</span> is the new x value, <span class="math inline">\(x\)</span> is the starting x value, <span class="math inline">\(\nabla_xf(x)\)</span> is the gradient with respect to <span class="math inline">\(x\)</span>, and <span class="math inline">\(\epsilon\)</span> is the step size. The step size is also known as the <em>learning rate</em>. Notation may vary, depending who is using it, but this concept is the same. We'll run through a simple example by hand below.</p>
<p>Let's choose the function <span class="math inline">\(y = (x-2)^2\)</span>, starting at the point <span class="math inline">\(x_0 = 4\)</span>, with a learning rate of 0.01. The derivative is <span class="math inline">\(\frac{dy}{dx}= 2(x-2)\)</span>. If you graph this function, you can see that the minimum is located at <span class="math inline">\(x = 2\)</span>.</p>
<p>On the first iteration of gradient descent: <span class="math inline">\(x_1 = x_0 - \epsilon \nabla_x f(x)\)</span></p>
<p><span class="math inline">\(x_1 = 4 - 0.01(2(4 - 2)) = 3.96\)</span></p>
<p>Second iteration of gradient descent: <span class="math inline">\(x_2 = x_1 - \epsilon \nabla_x f(x)\)</span></p>
<p><span class="math inline">\(x_2 = 3.96 - 0.01(2(3.96 - 2)) = 3.92\)</span></p>
<p>Third iteration of gradient descent: <span class="math inline">\(x_3 = x_2 - \epsilon \nabla_x f(x)\)</span></p>
<p><span class="math inline">\(x_3 = 3.92 - 0.01(2(3.92 - 2)) = 3.88\)</span></p>
<p>If we started at x = 0, this is what would happen: On the first iteration of gradient descent: <span class="math inline">\(x_1 = x_0 - \epsilon \nabla_x f(x)\)</span></p>
<p><span class="math inline">\(x_1 = 0 - 0.01(2(0 - 2)) = 0.04\)</span></p>
<p>Second iteration of gradient descent: <span class="math inline">\(x_2 = x_1 - \epsilon \nabla_x f(x)\)</span></p>
<p><span class="math inline">\(x_2 = 0.04 - 0.01(2(0.04 - 2)) = 0.079\)</span></p>
<p>Third iteration of gradient descent: <span class="math inline">\(x_3 = x_2 - \epsilon \nabla_x f(x)\)</span></p>
<p><span class="math inline">\(x_3 = 0.079 - 0.01(2(0.079 - 2)) = 0.117\)</span></p>
<p>We can see that with either start point, with each iteration of gradient descent we get closer to <span class="math inline">\(x=2\)</span>, the point of global minimum. We can stop performing iterations when the difference between consecutive iterations is less than a value that we set, for example 0.000001.</p>
<h3 id="choosing-a-learning-rate">Choosing a learning rate</h3>
<p><img src="https://cdn-images-1.medium.com/max/1600/0*QwE8M4MupSdqA3M4.png" /> Choosing the right learning rate is something to experiment with when doing gradient descent. With a too large learning rate, there is a chance that you'll step right past the minimum point and bounce around, missing the minimum and even ending up at a point with higher loss. With a too small learning rate, you might end up running many iterations and gradient descent will take forever.</p>
<div class="figure">
<img src="https://cdn-images-1.medium.com/max/1600/1*rcmvCjQvsxrJi8Y4HpGcCw.png" />

</div>
<p>The ideal learning rate will minimize the loss function in the fewest number of iterations necessary. In practice, you just want the loss function to decrease after every iteration. Finding this rate is a matter of expermentation and depends on the project, though common learning rates range from 0.0001 up to 1.</p>
<h3 id="why-do-gradient-descent">Why do gradient descent?</h3>
<p>But wait - haven't we found the x value that gives the minimum value of a function in calculus? Why even bother with gradient descent?</p>
<p>Long story short, finding the solutions to functions in higher dimensions (functions in terms of multiple variables) can often be very complicated. In these cases, gradient descent is a computationally faster way to find a solution.</p>
<p>Keep in mind that gradient descent doesn't always find the very best solution - the minimum it finds may not be the global minimum.</p>
<div class="figure">
<img src="https://www.superdatascience.com/wp-content/uploads/2018/09/Artificial_Neural_Networks_ANN_Stochastic_Gradient_Descent_Img2.png" />

</div>
<h3 id="application-in-feedforward-nets">Application in feedforward nets</h3>
<p>A network learns a function when its output is accurate to the output of a function. To do this, we use gradient descent to minimize the error of the network's output, by calculating the derivative of the error function with respect to the network weights and changing the weights such that the error decreases.</p>
<h4 id="image-sources">Image sources</h4>
<p>http://www.deepideas.net/deep-learning-from-scratch-iv-gradient-descent-and-backpropagation/</p>
<div class="sourceCode"><pre class="sourceCode python"><code class="sourceCode python"><span class="co"># SOURCE: wikipedia</span>
<span class="co"># From calculation, it is expected that the local minimum occurs at x=9/4</span>

cur_x <span class="op">=</span> <span class="dv">6</span> <span class="co"># The algorithm starts at x=6</span>
gamma <span class="op">=</span> <span class="fl">0.01</span> <span class="co"># step size multiplier</span>
precision <span class="op">=</span> <span class="fl">0.00001</span>
previous_step_size <span class="op">=</span> <span class="dv">1</span> 
max_iters <span class="op">=</span> <span class="dv">10000</span> <span class="co"># maximum number of iterations</span>
iters <span class="op">=</span> <span class="dv">0</span> <span class="co">#iteration counter</span>

df <span class="op">=</span> <span class="kw">lambda</span> x: <span class="dv">4</span> <span class="op">*</span> x<span class="op">**</span><span class="dv">3</span> <span class="op">-</span> <span class="dv">9</span> <span class="op">*</span> x<span class="op">**</span><span class="dv">2</span>

<span class="cf">while</span> previous_step_size <span class="op">&gt;</span> precision <span class="op">and</span> iters <span class="op">&lt;</span> max_iters:
    prev_x <span class="op">=</span> cur_x
    cur_x <span class="op">-=</span> gamma <span class="op">*</span> df(prev_x)
    previous_step_size <span class="op">=</span> <span class="bu">abs</span>(cur_x <span class="op">-</span> prev_x)
    iters<span class="op">+=</span><span class="dv">1</span>

<span class="bu">print</span>(<span class="st">&quot;The local minimum occurs at&quot;</span>, cur_x)
<span class="co">#The output for the above will be: (&#39;The local minimum occurs at&#39;, 2.2499646074278457)</span></code></pre></div>
<h1 id="backpropagation">Backpropagation</h1>
<p>Now that we know what <em>gradient descent</em> is, and we have an idea of why we would want the gradient of our <em>loss function</em> (<span class="math inline">\(C\)</span>) - how does one actually go about computing the gradient of a loss function?</p>
<p>The <em>backprogagation algorithm</em> is a fast algorithm to compute gradients - without it, none of the neural networks we use today would be able to function efficiently.</p>
<p>As touched on earlier, our neural network learns its weights and biases using gradient descent by minimizing loss. The key concept in backpropagation is the partial derivative of the loss function with respect to any weight <span class="math inline">\(w\)</span> or bias <span class="math inline">\(b\)</span> - <span class="math inline">\(\frac{\partial C}{\partial w}\)</span> or <span class="math inline">\(\frac{\partial C}{\partial b}\)</span> - which gives us an idea of how the loss will change when a bias or weight is changed. This is the power of understanding the backpropagation algorithm - it gives us the intuition necessary to manipulate the behaviour of our neural network by changing weights and biases. Understanding backpropagation gives you the key to open up the &quot;black box&quot; of a neural network.</p>
<h2 id="the-motivation">The Motivation</h2>
<p>Before diving into any of the math, lets try and motivate the <em>need</em> for backpropagation.</p>
<p>Our goal is to see how the loss function changes when we change a given weight. Sounds pretty simple, right? Just change the weight in question, <span class="math inline">\(w_i\)</span>, and calculate the loss <span class="math inline">\(C(w)\)</span>.</p>
<p>However, what seems like &quot;just&quot; changing a single weight quickly snowballs into a series of changes that effectively forces us to recalculate all of the weights in our networks. Let's break that down:</p>
<ol style="list-style-type: decimal">
<li><p>We change a single weight. <img src="http://neuralnetworksanddeeplearning.com/images/tikz22.png" alt="alt text" /></p></li>
<li><p>Th</p></li>
</ol>
<p>However, changing that single weight changes the output activation of the corresponding neuron, which then causes changes in all of the activations in the next layer, and the next, and so on. So, recalculating this loss function after changing a single weight requires us to pass through the entire neural network again!</p>
<p>To make matters worse, consider how many different weights and biases a single neural network can have - millions! All of a sudden, changing each weight/bias individually and recalculating the loss function each time seems a lot more daunting, right?</p>
<p>As we'll see, backpropagation allows us to calculate all of the partial derivatives in one pass forward and one pass backward through the neural network. So, instead of calculating the loss function a million times (which requires a million forward passes), we now just need to make a forward and a backward pass. Pretty great!</p>
<h2 id="the-four-fundamental-equations">The Four Fundamental Equations</h2>
<p>below, interpret <span class="math inline">\(j\)</span> and <span class="math inline">\(l\)</span> to mean we are working with <span class="math inline">\(j^{th}\)</span> neuron in layer <span class="math inline">\(l\)</span>, out of <span class="math inline">\(L\)</span> layers</p>
<p>also interpret <span class="math inline">\(s \odot t\)</span> to be the <em>elementwise</em> product of the vectors s and t</p>
<h3 id="equation-1---error-in-the-output-layer">Equation 1 - error in the output layer</h3>
<p><span class="math inline">\(\delta_j^L=\frac{\partial C}{\partial a_j^L} \sigma &#39; (z_j^L)\)</span></p>
<h4 id="what-does-this-mean">What does this mean?</h4>
<p><span class="math inline">\(\frac{\partial C}{\partial a_j^L}\)</span> - how fast does the loss change depending on the <span class="math inline">\(j^{th}\)</span> output activation (the activation of the output in the last layer)</p>
<p><span class="math inline">\(\sigma&#39;(z_j^L)\)</span>- how fast does the activation function <span class="math inline">\(\sigma\)</span> change at <span class="math inline">\(z_j^L\)</span></p>
<p>This is a component-wise equation - we can convert it to the equivalent matrix-based form easily</p>
<p>$^L = _a C $ <span class="math inline">\(\odot\)</span> <span class="math inline">\(\sigma&#39;(z^L)\)</span></p>
<p>but we will use the component-wise equation for convenience.</p>
<h3 id="equation-2---error-deltal-in-terms-of-the-error-in-the-next-layer-deltal1">Equation 2 - error <span class="math inline">\(\delta^l\)</span> in terms of the error in the next layer <span class="math inline">\(\delta^{l+1}\)</span></h3>
<p><span class="math inline">\(\delta^l=((w^{l+1})^T\delta^{l+1})\odot \sigma&#39;(z^l)\)</span></p>
<h4 id="what-does-this-mean-1">What does this mean?</h4>
<p><span class="math inline">\((w^{l+1})^T\)</span> - the transpose of the weight matrix <span class="math inline">\(w^{l+1}\)</span> for the <span class="math inline">\(l+1^{th}\)</span> layer</p>
<p>We can think of this as moving the error <span class="math inline">\(\delta^{l+1}\)</span> backward to the output of the <span class="math inline">\(l^{th}\)</span> layer by applying the transpose weight matrix, and then backward through the activation function in layer l (by taking the elementwise product <span class="math inline">\(\odot \sigma&#39;(z^l)\)</span>) to arrive at <span class="math inline">\(\delta^l\)</span></p>
<p>Thus, we can use Equation 1 to calculate <span class="math inline">\(\delta^L\)</span>, and then use Equation 2 to calculate all the other layers' errors by moving backwards through the network.</p>
<h3 id="equation-3---rate-of-change-of-cost-with-respect-to-bias">Equation 3 - rate of change of cost with respect to bias</h3>
<p><span class="math inline">\(\frac{\partial C}{\partial b^l_j} = \delta^l_j\)</span></p>
<h4 id="what-does-this-mean-2">What does this mean?</h4>
<p>The error <span class="math inline">\(\delta^l_j\)</span> is equal to the rate of change of cost with respect to bias $ $</p>
<p>Since given Equation 1 and 2, we can compute any <span class="math inline">\(\delta^l_j\)</span>, we can compute any <span class="math inline">\(\frac{\partial C}{\partial b^l_j}\)</span> as well</p>
<h3 id="equation-4---rate-of-change-of-cost-with-respect-to-weight">Equation 4 - rate of change of cost with respect to weight</h3>
<p>$ = a_k^{l-1}_j^l $</p>
<p>or</p>
<p><span class="math inline">\(\frac{\partial C}{\partial w}=a_{\text{in}}\delta_{\text{out}}\)</span></p>
<h4 id="what-does-this-mean-3">What does this mean?</h4>
<p>The rate of change of cost with respect to weight is the product of the activation of the neuron <em>input</em> to the weight <span class="math inline">\(w\)</span>, and the error of the neuron <em>output</em> from the weight <span class="math inline">\(w\)</span>.</p>
<h3 id="some-useful-intuition">Some Useful Intuition</h3>
<p>When the sigmoid function is approximately 0 or 1, it is very flat. In Equation 1, this gives us <span class="math inline">\(\sigma&#39;(z_j^L)\approx 0\)</span>. Essentially, a weight in the final layer will not change much - will &quot;learn slowly&quot; - if the output neuron is either low () or high () activation (in this case, we call the output neuron <em>saturated</em>).</p>
<p>The above logic also applies for the <span class="math inline">\(\sigma&#39;(z^l)\)</span> term in Equation 2, so this intuition can be extended to earlier layers.</p>
<p>Finally in Equation 4, if the activation of a neuron is small <span class="math inline">\((\approx 0)\)</span>, then the gradient term will also be small. Thus the weight will not change much during gradient descent - it will &quot;learn slowly&quot;.</p>
<h2 id="the-backpropagation-algorithm">The Backpropagation Algorithm</h2>
<ol style="list-style-type: decimal">
<li><p>Input x: Set the activation <span class="math inline">\(a^1\)</span> according to the input.</p></li>
<li><p>Feedforward: For each layer <span class="math inline">\(l = 2, 3, ..., L\)</span> compute <span class="math inline">\(z^l = w^l a^l-1 + b^l\)</span> and <span class="math inline">\(a^l = \sigma(z^l)\)</span></p></li>
<li><p>Output error <span class="math inline">\(\delta^L\)</span>: compute <span class="math inline">\(\delta^L\)</span> (Equation 1)</p></li>
<li><p>Backpropagate the error: compute <span class="math inline">\(\delta^l\)</span> for all the earlier layers (Equation 2)</p></li>
<li><p>Output: The gradient of the cost function is given by Equations 3 and 4.</p></li>
</ol>
<p>Step 4 is why the algorithm is called <em>back</em>propagation - we compute the error vectors backward, starting from the final layer.</p>
<h2 id="additional-resources">Additional Resources</h2>
<p>The material above is essentially a condensed version of Chapter 2 of Michael Nielsen's wonderful (and free!) <a href="http://neuralnetworksanddeeplearning.com/chap2.html">Neural Networks and Deep Learning textbook</a>.</p>
<p>For some visual intuition of what's going on in backpropagation, check out 3Blue1Brown's <a href="https://www.youtube.com/watch?v=Ilg3gGewQ5U">video</a>. For a little bit more math, he has a <a href="https://www.youtube.com/watch?v=tIeHLnjs5U8">follow up</a>.</p>
</body>
</html>
